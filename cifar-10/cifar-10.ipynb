{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "num_classes=10\n",
    "dropout_rate=0.4\n",
    "learning_rate=0.001\n",
    "\n",
    "def cnn_model(features, labels, mode):\n",
    "    input_data = tf.reshape(features, [-1, 3, 32, 32])\n",
    "    input_layer = tf.to_float(input_data)\n",
    "    \n",
    "    conv1_layer = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=40,\n",
    "        kernel_size=(5,5),\n",
    "        strides=(2,2),\n",
    "        padding='same',\n",
    "        data_format='channels_first',\n",
    "        activation=tf.nn.relu,\n",
    "        name='conv1')\n",
    "    \n",
    "    pool1_layer = tf.layers.max_pooling2d(\n",
    "        inputs=conv1_layer,\n",
    "        pool_size=(2,2),\n",
    "        strides=(2,2),\n",
    "        padding='same',\n",
    "        data_format='channels_first',\n",
    "        name='pool1')\n",
    "    \n",
    "    conv2_layer = tf.layers.conv2d(\n",
    "        inputs=pool1_layer,\n",
    "        filters=80,\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding='same',\n",
    "        data_format='channels_first',\n",
    "        activation=tf.nn.relu,\n",
    "        name='conv2')\n",
    "    \n",
    "    pool2_layer = tf.layers.max_pooling2d(\n",
    "        inputs=conv2_layer,\n",
    "        pool_size=(2,2),\n",
    "        strides=(2,2),\n",
    "        padding='same',\n",
    "        data_format='channels_first',\n",
    "        name='pool2')\n",
    "    pool2_flat = tf.reshape(pool2_layer, [-1, 80*4*4])\n",
    "    \n",
    "    dense1 = tf.layers.dense(\n",
    "        pool2_flat, \n",
    "        units=400, \n",
    "        activation=tf.nn.relu,\n",
    "        name='dense1')\n",
    "    \n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=dense1, \n",
    "        rate=dropout_rate, \n",
    "        training=mode==learn.ModeKeys.TRAIN,\n",
    "        name='dropout')\n",
    "    \n",
    "    class_prediction = tf.layers.dense(\n",
    "        inputs=dropout, \n",
    "        units=num_classes,\n",
    "        name='predict')\n",
    "    \n",
    "    training_op = None\n",
    "    loss = None\n",
    "    if mode != learn.ModeKeys.INFER: # ground-truth is unknown if we are inferring with new examples\n",
    "        ground_truth = tf.one_hot(indices=labels, depth=num_classes)\n",
    "#        print(class_prediction)\n",
    "        ground_truth = tf.reshape(ground_truth, [-1, 10])\n",
    "#        print(ground_truth)\n",
    "        loss = tf.losses.softmax_cross_entropy(onehot_labels=ground_truth,\n",
    "                                       logits=class_prediction)\n",
    "    \n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        training_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss, \n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            learning_rate=learning_rate,\n",
    "            optimizer=\"SGD\")\n",
    "    \n",
    "    predictions = {'classes': tf.argmax(input=class_prediction, axis=1), # the prediction\n",
    "                   'probabilties': tf.nn.softmax(class_prediction, name='softmax') }# predicted probabilities\n",
    "    \n",
    "    return model_fn_lib.ModelFnOps(\n",
    "        mode=mode, predictions=predictions, loss=loss, train_op=training_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cPickle\n",
    "import random\n",
    "\n",
    "random.seed(0x34c)\n",
    "\n",
    "label_names = dict()\n",
    "\n",
    "def unpickle(file_name):\n",
    "    with open(file_name, 'rb') as fo:\n",
    "        data = cPickle.load(fo)\n",
    "    return data\n",
    "\n",
    "def show_image_and_label(data, labels, index, interpolation=None):\n",
    "    global label_names\n",
    "    image = np.reshape(data[index], (3, 1024))\n",
    "    label = labels[index]\n",
    "    \n",
    "    image_reordered = []\n",
    "    for i in range(1024):\n",
    "        image_reordered.append((image[0][i], image[1][i], image[2][i]))\n",
    "        \n",
    "    image_reordered = np.reshape(image_reordered, (32, 32, 3))\n",
    "    plt.imshow(image_reordered, shape=(32,32), interpolation=interpolation)\n",
    "    print label_names[label]\n",
    "\n",
    "def load_cifar10_data(type=learn.ModeKeys.TRAIN, randomize=False):\n",
    "    if (type==learn.ModeKeys.TRAIN):\n",
    "        data1 = unpickle('dataset/cifar-10-batches-py/data_batch_1')\n",
    "        data2 = unpickle('dataset/cifar-10-batches-py/data_batch_2')\n",
    "        data3 = unpickle('dataset/cifar-10-batches-py/data_batch_3')\n",
    "        data4 = unpickle('dataset/cifar-10-batches-py/data_batch_4')\n",
    "        data5 = unpickle('dataset/cifar-10-batches-py/data_batch_5')\n",
    "\n",
    "        data = np.concatenate(\n",
    "            (data1['data'], \n",
    "             data2['data'], \n",
    "             data3['data'], \n",
    "             data4['data'], \n",
    "             data5['data']))\n",
    "        labels = np.concatenate(\n",
    "            (data1['labels'], \n",
    "             data2['labels'], \n",
    "             data3['labels'], \n",
    "             data4['labels'], \n",
    "             data5['labels']))\n",
    "    \n",
    "    else:\n",
    "        data1 = unpickle('dataset/cifar-10-batches-py/test_batch')\n",
    "        data = data1['data']\n",
    "        labels = data1['labels']\n",
    "    \n",
    "    if randomize:\n",
    "        c = list(zip(data, labels))\n",
    "        random.shuffle(c)\n",
    "        data, labels = zip(*c)\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "def main(unused_argv):\n",
    "    global label_names \n",
    "    label_names = unpickle('dataset/cifar-10-batches-py/batches.meta')['label_names']\n",
    "    data, labels = load_cifar10_data()\n",
    "    eval_data, eval_labels = load_cifar10_data(type=learn.ModeKeys.EVAL)\n",
    "    \n",
    "    classifier = learn.Estimator(\n",
    "        model_fn=cnn_model, \n",
    "        model_dir='tmp/cnn_convnet_model')\n",
    "    logging_hook = tf.train.LoggingTensorHook(\n",
    "        tensors={'probabilities': 'softmax'}, \n",
    "        every_n_iter=50)\n",
    "    classifier.fit(\n",
    "        x=data,\n",
    "        y=labels,\n",
    "        batch_size=100,\n",
    "        steps=20000,\n",
    "        #monitors=[logging_hook])\n",
    "        monitors=None)\n",
    "    \n",
    "    metrics={\n",
    "        'accuracy': learn.MetricSpec(metric_fn=tf.metrics.accuracy, prediction_key='classes'),\n",
    "    }\n",
    "    \n",
    "    eval_results = classifier.evaluate(\n",
    "        x=eval_data,\n",
    "        y=eval_labels,\n",
    "        metrics=metrics)\n",
    "    \n",
    "    print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
